Restoring modules from user's default
[2019-12-08 16:56:00,607 INFO] Extracting features...
[2019-12-08 16:56:00,800 INFO]  * number of source features: 0.
[2019-12-08 16:56:00,801 INFO]  * number of target features: 0.
[2019-12-08 16:56:00,801 INFO] Building `Fields` object...
[2019-12-08 16:56:00,801 INFO] Building & saving training data...
[2019-12-08 16:56:00,838 WARNING] Shards for corpus train already exist, won't be overwritten, pass the `-overwrite` option if you want to.
[2019-12-08 16:56:00,838 INFO] Building & saving validation data...
[2019-12-08 16:56:00,852 WARNING] Shards for corpus valid already exist, won't be overwritten, pass the `-overwrite` option if you want to.
[2019-12-08 16:56:04,902 INFO]  * src vocab size = 38
[2019-12-08 16:56:04,902 INFO]  * tgt vocab size = 42
[2019-12-08 16:56:04,902 INFO] Building model...
[2019-12-08 16:56:05,023 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(38, 500, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(42, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.3, inplace=False)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.3, inplace=False)
      (layers): ModuleList(
        (0): LSTMCell(1000, 500)
        (1): LSTMCell(500, 500)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=500, out_features=500, bias=False)
      (linear_out): Linear(in_features=1000, out_features=500, bias=False)
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=500, out_features=42, bias=True)
    (1): Cast()
    (2): LogSoftmax()
  )
)
[2019-12-08 16:56:05,023 INFO] encoder: 4027000
[2019-12-08 16:56:05,023 INFO] decoder: 5800042
[2019-12-08 16:56:05,023 INFO] * number of parameters: 9827042
[2019-12-08 16:56:05,064 INFO] Starting training on CPU, could be very slow
[2019-12-08 16:56:05,064 INFO] Start training loop and validate every 50 steps...
[2019-12-08 16:56:05,064 INFO] Loading dataset from Data/SVO_AN/prepro.train.0.pt
[2019-12-08 16:56:06,370 INFO] number of examples: 128841
[2019-12-08 16:58:44,542 INFO] Step 50/ 2014; acc:  12.48; ppl: 325.79; xent: 5.79; lr: 1.00000; 161/181 tok/s;    159 sec
[2019-12-08 16:58:44,557 INFO] Loading dataset from Data/SVO_AN/prepro.valid.0.pt
[2019-12-08 16:58:45,280 INFO] number of examples: 16105
[2019-12-08 17:06:16,157 INFO] Validation perplexity: 32.1767
[2019-12-08 17:06:16,164 INFO] Validation accuracy: 21.148
[2019-12-08 17:06:16,164 INFO] Model is improving ppl: inf --> 32.1767.
[2019-12-08 17:06:16,165 INFO] Model is improving acc: -inf --> 21.148.
[2019-12-08 17:08:43,927 INFO] Step 100/ 2014; acc:  19.88; ppl: 38.42; xent: 3.65; lr: 1.00000;  43/ 48 tok/s;    759 sec
[2019-12-08 17:08:43,939 INFO] Loading dataset from Data/SVO_AN/prepro.valid.0.pt
[2019-12-08 17:08:44,195 INFO] number of examples: 16105
slurmstepd: error: *** JOB 8939069 ON pg-node096 CANCELLED AT 2019-12-08T17:11:44 ***


###############################################################################
Peregrine Cluster
Job 8939069 for user 's2964007'
Finished at: Sun Dec  8 17:11:46 CET 2019

Job details:
============

Name                : onmt_test
User                : s2964007
Partition           : short
Nodes               : pg-node096
Cores               : 1
State               : CANCELLED,CANCELLED by 32964007
Submit              : 2019-12-08T16:55:44
Start               : 2019-12-08T16:55:45
End                 : 2019-12-08T17:11:45
Reserved walltime   : 00:20:00
Used walltime       : 00:16:00
Used CPU time       : 00:15:27 (efficiency: 96.60%)
% User (Computation): 59.13%
% System (I/O)      : 40.87%
Mem reserved        : 2000M/node
Max Mem used        : 639.29M (pg-node096)
Max Disk Write      : 71.68K (pg-node096)
Max Disk Read       : 49.68M (pg-node096)


Acknowledgements:
=================

Please see this page if you want to acknowledge Peregrine in your publications:

https://redmine.hpc.rug.nl/redmine/projects/peregrine/wiki/ScientificOutput

################################################################################
